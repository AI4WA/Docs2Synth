# Docs2Synth Configuration File

# Data folder settings - where to save all files
data:
  # Root data directory
  root_dir: ./data

  # Downloaded datasets
  datasets_dir: ./data/datasets

  # Processed data outputs
  processed_dir: ./data/processed

  # QA pairs
  qa_pairs_dir: ./data/qa_pairs

  # Models
  models_dir: ./models

  # Logs
  logs_dir: ./logs

# Logging configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # Logs always go to both console and file with line numbers
  level: INFO

  # File logging
  file:
    path: ./logs/docs2synth.log
    max_bytes: 10485760 # 10MB
    backup_count: 5

  # Third-party library log levels
  third_party:
    level: WARNING
    loggers:
      - urllib3
      - requests
      - transformers
      - torch
      - tensorflow
      - PIL
      - matplotlib
      - openai
      - anthropic
      - google.generativeai

# Preprocess configuration
# Configure default settings for document preprocessing
preprocess:
  # Default processor to use: paddleocr, pdfplumber, or easyocr
  processor: paddleocr

  # Default OCR language (e.g., en, zh, fr)
  # For paddleocr: en, ch, japan, korean, etc.
  # For easyocr: en, ch_sim, fr, etc.
  lang: en

  # Default device for OCR inference: cpu, gpu, cuda, or null (auto-detect)
  # If null, will auto-select GPU when available
  device: null

  input_dir: ./data/datasets/docs2synth-dev/docs2synth-dev/images/
  # Default output directory (if not specified, uses data.processed_dir)
  output_dir: ./data/processed
  # Optional: Downscale high-resolution images before processing
  image_resize:
    enabled: false
    max_long_edge: 1280

# Agent/LLM configuration (provider-driven presets)
agent:
  # Choose active provider here: openai | anthropic | gemini | doubao | ollama | huggingface | vllm
  provider: openai

  # Centralized API keys used to backfill per-provider configs
  keys:
    openai_api_key: "sk-proj-YOUR-OPENAI-API-KEY"
    anthropic_api_key: "sk-ant-YOUR-ANTHROPIC-API-KEY"
    google_api_key: "YOUR-GOOGLE-API-KEY"
    doubao_api_key: "YOUR-DOUBAO-API-KEY"
    huggingface_token: "YOUR-HF-TOKEN" # For gated models on Hugging Face

  # Provider-specific configuration blocks. Switch provider by changing agent.provider.
  openai:
    model: gpt-4o-mini
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    frequency_penalty: 0
    presence_penalty: 0
    stop: null
    # Optional explicit key override (otherwise uses agent.keys.openai_api_key)
    # api_key: "sk-proj-..."

  anthropic:
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    stop_sequences: null
    # api_key: "sk-ant-..."

  gemini:
    model: gemini-1.5-pro
    temperature: 0.7
    max_output_tokens: 1000
    top_p: 1
    top_k: 40
    # api_key: "..."

  doubao:
    model: doubao-pro-32k
    base_url: https://ark.cn-beijing.volces.com/api/v3
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    # api_key: "..."

  ollama:
    model: llama3
    host: http://localhost:11434
    temperature: 0.7
    top_p: 1

  huggingface:
    model: meta-llama/Llama-2-7b-chat-hf
    device: auto # auto | cpu | cuda
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    load_in_4bit: false
    # If not provided here, will backfill from agent.keys.huggingface_token
    # hf_token: YOUR-HF-TOKEN

  vllm:
    # vLLM provider for high-performance local LLM inference
    # Uses OpenAI-compatible API server mode
    # Start server: docs2synth agent vllm-server
    # Or manually: python -m vllm.entrypoints.openai.api_server --model <model_name>

    # Model name/identifier (Hugging Face model ID or local path)
    model: meta-llama/Llama-2-7b-chat-hf

    # vLLM server URL (default: http://localhost:8000/v1)
    base_url: http://localhost:8000/v1

    # Optional API key (vLLM typically doesn't require auth)
    # api_key: null

    # ---- Server Startup Parameters (used by 'docs2synth agent vllm-server') ----
    # These parameters are used when starting the vLLM server
    trust_remote_code: true  # Required for some models (e.g., Qwen, Phi)
    max_model_len: 4096  # Maximum sequence length
    gpu_memory_utilization: 0.9  # GPU memory utilization (0.0-1.0)
    # tensor_parallel_size: 1  # Number of GPUs to use for tensor parallelism

    # ---- Generation Parameters ----
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    frequency_penalty: 0
    presence_penalty: 0
    stop: null

    # ---- Example configurations for different use cases ----

    # Example 1: Vision-Language Model
    # model: Qwen/Qwen-VL-Chat
    # trust_remote_code: true
    # max_model_len: 2048  # Adjust based on GPU memory
    # gpu_memory_utilization: 0.9

    # Example 2: Large Model with Multi-GPU
    # model: meta-llama/Llama-2-70b-chat-hf
    # tensor_parallel_size: 4  # Use 4 GPUs
    # gpu_memory_utilization: 0.95
    # max_model_len: 4096

    # Example 3: Remote vLLM Server
    # model: meta-llama/Llama-2-7b-chat-hf
    # base_url: http://remote-server:8000/v1
    # api_key: your-api-key-if-needed

# QA Generation configuration
# Configure multiple QA generation strategies with different providers, models, and prompts

# --- QA Generation configuration ---
# Configure multiple QA generation strategies with different providers, models, and prompts
qa:
  # List of QA generation strategies to use
  # Each strategy can have its own provider, model, prompt_template, and generation parameters
  #
  # Grouping strategies with group_id:
  # - Strategies with the same group_id form a group
  # - Each group should have one semantic strategy (generates base questions)
  # - Each group can have multiple layout_aware and logical_aware strategies
  # - layout_aware and logical_aware strategies transform the semantic question from the same group
  # - If group_id is not specified, strategies are grouped by strategy type (backward compatible)
  #
  # Testing all 6 providers for semantic strategy
  # - strategy: semantic
  #   provider: openai
  #   model: gpt-4o
  #   prompt_template: |
  #     Context: {context}
  #     Based on the above context and target document image, 
  #     generate a human-asked SHORT question (output question only) 
  #     of which answer is exactly same as "{target}"
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_openai_gpt-4o

  # Quick switch: Change 'provider' below to switch between providers
  # - ollama: Vision model, most stable (qwen3-vl:8b already installed)
  # - vllm: Text only, fastest (Llama-2-7b-chat-hf) - CURRENT
  # - huggingface: Vision model, most flexible (Qwen2-VL-7B-Instruct)
  - strategy: semantic
    provider: vllm  # Using vLLM server (must be running)
    prompt_template: |
      Context: {context}
      Based on the above context and target document image,
      generate a human-asked SHORT question (output question only)
      of which answer is exactly same as "{target}"
    temperature: 0.7
    max_tokens: 256
    group_id: semantic_vllm_llama2-7b

  # - strategy: semantic
  #   provider: anthropic
  #   prompt_template: |
  #     Context: {context}
  #     Based on the above context and target document image, 
  #     generate a human-asked SHORT question (output question only) 
  #     of which answer is exactly same as "{target}"
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_anthropic_claude-sonnet-4-5-20250929
  # - strategy: semantic
  #   provider: gemini
  #   prompt_template: |
  #     Context: {context}
  #     Based on the above context and target document image, 
  #     generate a human-asked SHORT question (output question only) 
  #     of which answer is exactly same as "{target}"
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_gemini_default

  # - strategy: semantic
  #   provider: doubao
  #   prompt_template: |
  #     Context: {context}
  #     Based on the above context and target document image, 
  #     generate a human-asked SHORT question (output question only) 
  #     of which answer is exactly same as "{target}"
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_doubao_default

  # - strategy: semantic
  #   provider: ollama
  #   prompt_template: |
  #     Context: {context}
  #     Based on the above context and target document image, 
  #     generate a human-asked SHORT question (output question only) 
  #     of which answer is exactly same as "{target}"
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_ollama_default

  # - strategy: semantic
  #   provider: huggingface
  #   prompt_template: |
  #     Context: {context}
  #     Based on the above context and target document image, 
  #     generate a human-asked SHORT question (output question only) 
  #     of which answer is exactly same as "{target}"
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_huggingface_default

  # Testing all 6 providers for layout_aware strategy
  # These layout_aware strategies belong to their respective semantic groups
  - strategy: layout_aware
    provider: vllm
    prompt_template: |
      Change the question {question} to a very short question 
      about finding the position of the answer from the input document image.
      For example, where is the answer of xx located?
    temperature: 0.7
    max_tokens: 256
    group_id: semantic_vllm_llama2-7b

  # - strategy: layout_aware
  #   provider: anthropic
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the position of the answer from the input document image.
  #     For example, where is the answer of xx located?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_anthropic_claude-sonnet-4-5-20250929

  # - strategy: layout_aware
  #   provider: gemini
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the position of the answer from the input document image.
  #     For example, where is the answer of xx located?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_gemini_default

  # - strategy: layout_aware
  #   provider: doubao
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the position of the answer from the input document image.
  #     For example, where is the answer of xx located?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_doubao_default

  # - strategy: layout_aware
  #   provider: ollama
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the position of the answer from the input document image.
  #     For example, where is the answer of xx located?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_ollama_default

  # - strategy: layout_aware
  #   provider: huggingface
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the position of the answer from the input document image.
  #     For example, where is the answer of xx located?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_huggingface_default

  # Testing all 6 providers for logical_aware strategy
  # These logical_aware strategies belong to their respective semantic groups
  - strategy: logical_aware
    provider: vllm
    prompt_template: |
      Change the question {question} to a very short question 
      about finding the belonging sections of the answer from the input document.
      For example, which section you could find the information about the xx?
    temperature: 0.7
    max_tokens: 256
    group_id: semantic_vllm_llama2-7b

  # - strategy: logical_aware
  #   provider: anthropic
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the belonging sections of the answer from the input document.
  #     For example, which section you could find the information about the xx?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_anthropic_claude-sonnet-4-5-20250929

  # - strategy: logical_aware
  #   provider: gemini
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the belonging sections of the answer from the input document.
  #     For example, which section you could find the information about the xx?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_gemini_default

  # - strategy: logical_aware
  #   provider: doubao
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the belonging sections of the answer from the input document.
  #     For example, which section you could find the information about the xx?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_doubao_default

  # - strategy: logical_aware
  #   provider: ollama
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the belonging sections of the answer from the input document.
  #     For example, which section you could find the information about the xx?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_ollama_default

  # - strategy: logical_aware
  #   provider: huggingface
  #   prompt_template: |
  #     Change the question {question} to a very short question 
  #     about finding the belonging sections of the answer from the input document.
  #     For example, which section you could find the information about the xx?
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_huggingface_default

# --- QA Verifiers configuration ---
verifiers:
  # Correctness verifier: checks if answer matches question
  - verifier_type: correctness
    provider: vllm

    prompt_template: |
      Ignore the context information and domain knowledge (e.g. ABN or ACN/ARSN).
      Just consider whether '{answer}' could be the expected answer to the question '{question}'.
      Output format (JSON with double quotes): {{"Response": "Yes/No", "Explanation": "xxx"}}.
    temperature: 0.7
    max_tokens: 256

  # Meaningful verifier: checks if target was entered by user (not part of template)
  - verifier_type: meaningful
    provider: vllm

    prompt_template: |
      Question: {question}
      Answer: {answer}
      Context: {context}
      Based on the document context and image, determine if the question and answer pair is meaningful. 
      A meaningful QA pair should:
      1. Have a clear, answerable question
      2. Have an answer that can be found or inferred from the document
      3. Be useful for understanding the document content
      Output format (JSON with double quotes): {{"Response": "Yes/No", "Explanation": "xxx"}}.
    temperature: 0.7
    max_tokens: 256


# --- Retriever configuration ---
retriever:
  # Identifier to scope retriever preprocessing, checkpoints, and model artifacts
  run_id: default

  # Preprocessed training data (DataLoader pickle file)
  preprocessed_data_path: ./data/retriever/{run_id}/preprocessed_train.pkl

  # Directory for training checkpoints
  checkpoint_dir: ./models/retriever/{run_id}/checkpoints/

  # Final trained model path
  model_path: ./models/retriever/{run_id}/final_model.pth

  # Default training hyperparameters
  learning_rate: 1e-5
  batch_size: 8
  epochs: 10
  save_every: 2  # Save checkpoint every N epochs
