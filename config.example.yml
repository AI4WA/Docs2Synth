# Docs2Synth Configuration File

# Data folder settings - where to save all files
data:
  # Root data directory
  root_dir: ./data

  # Downloaded datasets
  datasets_dir: ./data/datasets

  # Processed data outputs
  processed_dir: ./data/processed

  # QA pairs
  qa_pairs_dir: ./data/qa_pairs

  # Models
  models_dir: ./models

  # Logs
  logs_dir: ./logs

# Logging configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # Logs always go to both console and file with line numbers
  level: INFO

  # File logging
  file:
    path: ./logs/docs2synth.log
    max_bytes: 10485760 # 10MB
    backup_count: 5

  # Third-party library log levels
  third_party:
    level: WARNING
    loggers:
      - urllib3
      - requests
      - transformers
      - torch
      - tensorflow
      - PIL
      - matplotlib
      - openai
      - anthropic
      - google.generativeai

# Agent/LLM configuration (provider-driven presets)
agent:
  # Choose active provider here: openai | anthropic | gemini | doubao | ollama | huggingface
  provider: openai

  # Centralized API keys used to backfill per-provider configs
  keys:
    openai_api_key: "sk-proj-YOUR-OPENAI-API-KEY"
    anthropic_api_key: "sk-ant-YOUR-ANTHROPIC-API-KEY"
    google_api_key: "YOUR-GOOGLE-API-KEY"
    doubao_api_key: "YOUR-DOUBAO-API-KEY"
    huggingface_token: "YOUR-HF-TOKEN"  # For gated models on Hugging Face

  # Provider-specific configuration blocks. Switch provider by changing agent.provider.
  openai:
    model: gpt-4o-mini
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    frequency_penalty: 0
    presence_penalty: 0
    stop: null
    # Optional explicit key override (otherwise uses agent.keys.openai_api_key)
    # api_key: "sk-proj-..."

  anthropic:
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    stop_sequences: null
    # api_key: "sk-ant-..."

  gemini:
    model: gemini-1.5-pro
    temperature: 0.7
    max_output_tokens: 1000
    top_p: 1
    top_k: 40
    # api_key: "..."

  doubao:
    model: doubao-pro-32k
    base_url: https://ark.cn-beijing.volces.com/api/v3
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    # api_key: "..."

  ollama:
    model: llama3
    host: http://localhost:11434
    temperature: 0.7
    top_p: 1

  huggingface:
    model: meta-llama/Llama-2-7b-chat-hf
    device: auto     # auto | cpu | cuda
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    load_in_4bit: false
    # If not provided here, will backfill from agent.keys.huggingface_token
    # hf_token: YOUR-HF-TOKEN
