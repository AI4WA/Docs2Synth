# Docs2Synth Configuration File

# Data folder settings - where to save all files
data:
  # Root data directory
  root_dir: ./data

  # Downloaded datasets
  datasets_dir: ./data/datasets

  # Processed data outputs
  processed_dir: ./data/processed

  # QA pairs
  qa_pairs_dir: ./data/qa_pairs

  # Models
  models_dir: ./models

  # Logs
  logs_dir: ./logs

# Logging configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  # Logs always go to both console and file with line numbers
  level: INFO

  # File logging
  file:
    path: ./logs/docs2synth.log
    max_bytes: 10485760 # 10MB
    backup_count: 5

  # Third-party library log levels
  third_party:
    level: WARNING
    loggers:
      - urllib3
      - requests
      - transformers
      - torch
      - tensorflow
      - PIL
      - matplotlib
      - openai
      - anthropic
      - google.generativeai

# Preprocess configuration
# Configure default settings for document preprocessing
preprocess:
  # Default processor to use: paddleocr, pdfplumber, or easyocr
  processor: paddleocr

  # Default OCR language (e.g., en, zh, fr)
  # For paddleocr: en, ch, japan, korean, etc.
  # For easyocr: en, ch_sim, fr, etc.
  lang: en

  # Default device for OCR inference: cpu, gpu, cuda, or null (auto-detect)
  # If null, will auto-select GPU when available
  device: null

  input_dir: ./data/datasets/docs2synth-dev/docs2synth-dev/images/
  # Default output directory (if not specified, uses data.processed_dir)
  output_dir: ./data/processed

# Agent/LLM configuration (provider-driven presets)
agent:
  # Choose active provider here: openai | anthropic | gemini | doubao | ollama | huggingface
  provider: openai

  # Centralized API keys used to backfill per-provider configs
  keys:
    openai_api_key: "sk-proj-YOUR-OPENAI-API-KEY"
    anthropic_api_key: "sk-ant-YOUR-ANTHROPIC-API-KEY"
    google_api_key: "YOUR-GOOGLE-API-KEY"
    doubao_api_key: "YOUR-DOUBAO-API-KEY"
    huggingface_token: "YOUR-HF-TOKEN" # For gated models on Hugging Face

  # Provider-specific configuration blocks. Switch provider by changing agent.provider.
  openai:
    model: gpt-4o-mini
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    frequency_penalty: 0
    presence_penalty: 0
    stop: null
    # Optional explicit key override (otherwise uses agent.keys.openai_api_key)
    # api_key: "sk-proj-..."

  anthropic:
    model: claude-3-5-sonnet-20241022
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    stop_sequences: null
    # api_key: "sk-ant-..."

  gemini:
    model: gemini-1.5-pro
    temperature: 0.7
    max_output_tokens: 1000
    top_p: 1
    top_k: 40
    # api_key: "..."

  doubao:
    model: doubao-pro-32k
    base_url: https://ark.cn-beijing.volces.com/api/v3
    temperature: 0.7
    max_tokens: 1000
    top_p: 1
    # api_key: "..."

  ollama:
    model: llama3
    host: http://localhost:11434
    temperature: 0.7
    top_p: 1

  huggingface:
    model: meta-llama/Llama-2-7b-chat-hf
    device: auto # auto | cpu | cuda
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.95
    load_in_8bit: false
    load_in_4bit: false
    # If not provided here, will backfill from agent.keys.huggingface_token
    # hf_token: YOUR-HF-TOKEN

# QA Generation configuration
# Configure multiple QA generation strategies with different providers, models, and prompts
qa:
  # List of QA generation strategies to use
  # Each strategy can have its own provider, model, prompt_template, and generation parameters
  # 
  # Grouping strategies with group_id:
  # - Strategies with the same group_id form a group
  # - Each group should have one semantic strategy (generates base questions)
  # - Each group can have multiple layout_aware and logical_aware strategies
  # - layout_aware and logical_aware strategies transform the semantic question from the same group
  # - If group_id is not specified, strategies are grouped by strategy type (backward compatible)
  #
  # Example group: semantic_openai_gpt-4o-mini
  - strategy: semantic
    provider: openai
    model: gpt-4o-mini
    prompt_template: |
      Context: {context}
      Based on the above context and target document image, 
      generate a human-asked SHORT question (output question only) 
      of which answer is exactly same as "{target}"
    temperature: 0.7
    max_tokens: 256
    group_id: semantic_openai_gpt-4o-mini

  - strategy: layout_aware
    provider: openai
    model: gpt-4o-mini
    prompt_template: |
      Change the question {question} to a very short question 
      about finding the position of the answer from the input document image.
      For example, where is the answer of xx located?
    temperature: 0.7
    max_tokens: 256
    group_id: semantic_openai_gpt-4o-mini

  - strategy: logical_aware
    provider: openai
    model: gpt-4o-mini
    prompt_template: |
      Change the question {question} to a very short question 
      about finding the belonging sections of the answer from the input document.
      For example, which section you could find the information about the xx?
    temperature: 0.5
    max_tokens: 256
    group_id: semantic_openai_gpt-4o-mini

  # Example: Another group with different provider
  # - strategy: semantic
  #   provider: gemini
  #   model: gemini-1.5-pro
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_gemini_gemini-1.5-pro
  #
  # - strategy: layout_aware
  #   provider: gemini
  #   model: gemini-1.5-pro
  #   temperature: 0.7
  #   max_tokens: 256
  #   group_id: semantic_gemini_gemini-1.5-pro

# QA Verification configuration
# Configure verifiers to validate QA pairs
verifiers:
  # Correctness verifier: checks if answer matches question
  - verifier_type: correctness
    provider: openai
    model: gpt-4o-mini
    prompt_template: |
      Ignore the context information and domain knowledge (e.g. ABN or ACN/ARSN).
      Just consider whether '{answer}' could be the expected answer to the question '{question}'.
      Output format (JSON with double quotes): {{"Response": "Yes/No", "Explanation": "xxx"}}.
    temperature: 0.7
    max_tokens: 256

  # Meaningful verifier: checks if target was entered by user (not part of template)
  - verifier_type: meaningful
    provider: openai
    model: gpt-4o-mini
    prompt_template: |
      Question: {question}
      Answer: {answer}
      Context: {context}
      Based on the document context and image, determine if the question and answer pair is meaningful. 
      A meaningful QA pair should:
      1. Have a clear, answerable question
      2. Have an answer that can be found or inferred from the document
      3. Be useful for understanding the document content
      Output format (JSON with double quotes): {{"Response": "Yes/No", "Explanation": "xxx"}}.
    temperature: 0.7
    max_tokens: 256
