{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Wrapper Demo - LLM Provider Abstraction\n",
    "\n",
    "This notebook demonstrates how to use Docs2Synth's agent wrapper to seamlessly switch between different LLM providers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DOCS2SYNTH_CONFIG\"] = \"../../config.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docs2synth.agent import AgentWrapper, QAGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. View Available Providers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docs2synth.agent.providers import PROVIDER_REGISTRY\n",
    "\n",
    "print(\"Available Providers:\")\n",
    "for name, provider_class in sorted(PROVIDER_REGISTRY.items()):\n",
    "    print(f\"  - {name}: {provider_class.__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Usage - Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentWrapper(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "response = agent.generate(\"Explain what artificial intelligence is in one sentence\")\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Token Usage: {response.usage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. QA Generation - Main Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example document content\n",
    "content = \"\"\"\n",
    "Python is a high-level programming language first released by Guido van Rossum in 1991.\n",
    "It is known for its simple and readable syntax, making it ideal for beginners.\n",
    "Python supports multiple programming paradigms including object-oriented, imperative,\n",
    "functional, and procedural programming. It has a large standard library and a rich\n",
    "ecosystem of third-party packages.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Example Content:\")\n",
    "print(content.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate QA pair (automatically uses JSON format)\n",
    "# Uncomment and set your API key to run\n",
    "\n",
    "generator = QAGenerator(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "qa_pair = generator.generate_qa_pair(content)\n",
    "\n",
    "print(f\"\\nQuestion: {qa_pair['question']}\")\n",
    "print(f\"Answer: {qa_pair['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch QA Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple document contents\n",
    "contents = [\n",
    "    \"Machine learning is a subset of artificial intelligence focused on enabling computers to learn from data.\",\n",
    "    \"Deep learning uses multi-layer neural networks to simulate how the human brain works.\",\n",
    "    \"Natural language processing enables computers to understand and generate human language.\"\n",
    "]\n",
    "\n",
    "# Batch generation\n",
    "generator = QAGenerator(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "qa_pairs = generator.generate_qa_pairs(contents)\n",
    "\n",
    "for i, qa in enumerate(qa_pairs, 1):\n",
    "    print(f\"\\n=== QA Pair {i} ===\")\n",
    "    print(f\"Question: {qa['question']}\")\n",
    "    print(f\"Answer: {qa['answer']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Switching Between Different Providers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All providers use the same interface - just change the provider parameter to switch\n",
    "\n",
    "# OpenAI\n",
    "generator = QAGenerator(provider=\"openai\")\n",
    "\n",
    "# Anthropic Claude\n",
    "generator = QAGenerator(provider=\"anthropic\")\n",
    "\n",
    "# Google Gemini\n",
    "generator = QAGenerator(provider=\"gemini\")\n",
    "\n",
    "# Doubao (Bean)\n",
    "generator = QAGenerator(\n",
    "    provider=\"doubao\"\n",
    ")\n",
    "\n",
    "# Local Ollama (requires local ollama serve running)\n",
    "generator = QAGenerator(provider=\"ollama\")\n",
    "\n",
    "# Local Hugging Face\n",
    "generator = QAGenerator(\n",
    "    provider=\"huggingface\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using JSON Format Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use AgentWrapper directly and specify JSON format\n",
    "agent = AgentWrapper(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = agent.generate(\n",
    "    \"Return a JSON object with 'name' and 'age' fields, name='John', age=25\",\n",
    "    response_format=\"json\"\n",
    ")\n",
    "\n",
    "print(\"JSON Response:\", response.content)\n",
    "data = json.loads(response.content)\n",
    "print(\"Parsed Data:\", data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Chat Interface Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentWrapper(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a programming language.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are its characteristics?\"}\n",
    "]\n",
    "\n",
    "response = agent.chat(messages)\n",
    "print(\"Response:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Runtime Provider Switching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentWrapper(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "print(f\"Current Provider: {agent.provider_type}, Model: {agent.model}\")\n",
    "\n",
    "# Switch to Ollama\n",
    "agent.switch_provider(\"ollama\", model=\"llama2\")\n",
    "print(f\"Switched Provider: {agent.provider_type}, Model: {agent.model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. QA Generation with Verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = QAGenerator(provider=\"openai\", model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Generate and verify (meaningfulness + correctness)\n",
    "qa_pair = generator.generate_with_verification(\n",
    "    content,\n",
    "    meaningful_check=True,\n",
    "    correctness_check=True\n",
    ")\n",
    "\n",
    "if qa_pair:\n",
    "    print(\"✓ QA pair passed verification\")\n",
    "    print(f\"Question: {qa_pair['question']}\")\n",
    "    print(f\"Answer: {qa_pair['answer']}\")\n",
    "else:\n",
    "    print(\"✗ QA pair failed verification\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "1. **API Keys**: Make sure to set the correct API keys in `config.yml` or use environment variables\n",
    "2. **Local Models**: Ollama and Hugging Face require local setup or model installation\n",
    "3. **JSON Mode**: QAGenerator automatically uses JSON mode to ensure responses can be parsed correctly\n",
    "4. **Provider Differences**: Different providers implement JSON mode differently (native support vs prompt engineering)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
